# -*- coding: utf-8 -*-
"""SVM_pjfinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nn22pdoyHXD2ZJ2X3KKWvVeeEV8h3n5B
"""

import pandas as pd
import numpy as np
import re
import nltk.corpus
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from keras.utils import pad_sequences
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
#from keras.layers import Dense, Embedding,Flatten,LSTM,Bidirectional
from sklearn.model_selection import train_test_split
import sklearn.model_selection as model_selection
from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.over_sampling import RandomOverSampler
from sklearn import svm,datasets
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score,f1_score,classification_report,confusion_matrix

File_path = "https://docs.google.com/spreadsheets/d/e/2PACX-1vR9J1s8wQy36wmUWJCWJ3dXBXHY7Sux4QMmSYmw1G86SWOytiMqkP6dRlx6TVP1CtDIuifMC-DHHV_2/pub?gid=1475446522&single=true&output=csv"
#data = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/5_stock_data.xlsx')
#data.head()
data = pd.read_csv(File_path)
data

duplicated_values = data.duplicated().sum()
print( "Data Duplicated: ", duplicated_values)

missing_values = data.isna().sum()
print("Missing Values:")
print(missing_values)

data = data.drop_duplicates()
data

format = r"(@\[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?"
text = data['Text'].astype(str)

cleaned_texts = text.str.replace(format, "", regex=True)

print(cleaned_texts)

def convert_labels_to_binary(label):
    if label == -1:
        return 0
    elif label == 1:
        return 1
labels = data['Sentiment']
labels = [convert_labels_to_binary(label) for label in labels]
labels

stop = stopwords.words('english')
lemmatizer = WordNetLemmatizer()

texts = cleaned_texts.values

def remove_stopwords(text):
    words = word_tokenize(text)
    cleaned_words = [word for word in words if word.lower() not in stop]
    lemmatized_words = [lemmatizer.lemmatize(word) for word in cleaned_words]
    return " ".join(lemmatized_words)

def convert_labels_to_binary(label):
    #if label == -1:
        #return 0
    #elif label == 1:
        #return 1
#labels = data['Sentiment']
#labels = [convert_labels_to_binary(label) for label in labels]
#y = np.array(labels)
#y

cleaned_stopw = [remove_stopwords(text) for text in texts]
print(cleaned_stopw)

print(cleaned_stopw)
print(labels )

tokenizer = Tokenizer()
 tokenizer.fit_on_texts(cleaned_stopw)
 tts = tokenizer.texts_to_sequences(cleaned_stopw)
 max_length = max([len(s) for s in tts])
 print("max sentense's length = " , max_length)
 X = pad_sequences(tts, maxlen=max_length, padding="post")
 print("Sequence Padding at maxlen words, post padding:\n", X)
  vocab_size = len(tokenizer.word_index) +1
 print("vocab size : ",vocab_size)

tfidf_vectorizer = TfidfVectorizer()
X = tfidf_vectorizer.fit_transform(cleaned_stopw)

oversampler = RandomOverSampler(sampling_strategy='minority')
X_resampled, y_resampled = oversampler.fit_resample(X, labels)
y = np.array(y_resampled )
y

X_train,X_test,y_train,y_test = train_test_split(X_resampled,y,test_size = 0.20,random_state = 42)

# สร้างโมเดล Support Vector Machine ใช้ kernel แบบ RBF (Radial Basis Function)
rbf = svm.SVC(kernel='rbf', gamma=0.5, C=1)
# สร้างโมเดล Support Vector Machine ใช้ kernel แบบ poly (polynomial function)
poly = svm.SVC(kernel='poly', degree=3, C=1)

#train

rbf.fit(X_train, y_train)
poly.fit(X_train, y_train)

#predict

poly_pred = poly.predict(X_test)
rbf_pred = rbf.predict(X_test)
print("Predict (RBF):", rbf_pred)
print("Predict (Poly):", poly_pred)

#ค่าความแม่นยำ
accuracy_rbf = accuracy_score(y_test, rbf_pred)
accuracy_poly = accuracy_score(y_test, poly_pred)

print("Accuracy (RBF):", accuracy_rbf)
print("Accuracy (Poly):", accuracy_poly)

'''
poly_accuracy = accuracy_score(y_test, poly_pred)
poly_f1 = f1_score(y_test, poly_pred, average='weighted')
print('Accuracy (Polynomial Kernel): ', "%.2f" % (poly_accuracy*100))
print('F1 (Polynomial Kernel): ', "%.2f" % (poly_f1*100))
'''

print(classification_report(y_test,rbf_pred))

print(classification_report(y_test,poly_pred))

'''
rbf_accuracy = accuracy_score(y_test, rbf_pred)
rbf_f1 = f1_score(y_test, rbf_pred, average='weighted')
print('Accuracy (RBF Kernel): ', "%.2f" % (rbf_accuracy*100))
print('F1 (RBF Kernel): ', "%.2f" % (rbf_f1*100))
'''

# rbf_pred = rbf.predict(X_test)
# rbf_pred = rbf_pred > 0.5
confusion_mat = confusion_matrix(y_test,rbf_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1'], yticklabels=['0', '1'])
plt.xlabel('True')
plt.ylabel('Predicted')
plt.title('Confusion Matrix')
plt.show()

target_names = ['0', '1']
report = classification_report(y_test, rbf_pred, target_names=target_names)
print(report)

# poly_pred = poly.predict(X_test)
# poly_pred = poly_pred > 0.5
confusion_mat = confusion_matrix(poly_pred,y_test)
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1'], yticklabels=['0', '1'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

target_names = ['0', '1']
report = classification_report(y_test, poly_pred, target_names=target_names)
print(report)

'''
new_text = "long MS small"
new_text_tokenized = word_tokenize(new_text) #keep_whitespace=False)
new_text_tfidf = tfidf_vectorizer.transform([" ".join(new_text_tokenized)])
y_pred_rbf = rbf.predict(new_text_tfidf)
y_pred_poly = poly.predict(new_text_tfidf)
print('Predict (RBF):', y_pred_rbf)
print('Predict (Poly):', y_pred_poly)
'''

#predict kernel rbf
new_text = "SD this is the ammo to help fire Tom Ward, was needed imho... unfortunately once again at the shareholders expense. NG SO"
new_text_tfidf = tfidf_vectorizer.transform([new_text])
predictions = rbf.predict(new_text_tfidf)
predicted_label_rbf = 1 if predictions > 0.5 else 0
predicted_label_rbf

#predict kernel polynomial
new_text = "SD this is the ammo to help fire Tom Ward, was needed imho... unfortunately once again at the shareholders expense. NG SO"
new_text_tfidf = tfidf_vectorizer.transform([new_text])
predictions = poly.predict(new_text_tfidf)
predicted_label_poly = 1 if predictions > 0.5 else 0
predicted_label_poly

import joblib

# เซฟโมเดล SVM แบบ RBF
with open('svm_rbf_model.pkl', 'wb') as file:
    joblib.dump(rbf, file)

# เซฟโมเดล SVM แบบ Poly
with open('svm_poly_model.pkl', 'wb') as file:
    joblib.dump(poly, file)

with open('svm_vectorizer.pkl', 'wb') as file:
    joblib.dump(tfidf_vectorizer, file)

#แก้ไข code เพื่อให้ส่งข้อมูลตัดคำไปทำ vectorizer ได้ถูกต้อง และจะทำให้ classify ได้ถูกต้อง
#new_text = "Kickers on my watchlist XIDE TIT SOQ PNK CPW BPZ AJ  trade method 1 or method 2, see prev posts"

#y_preds = rbf.predict(new_text)
#print('predict : ',y_preds)

# 4. Create an SVM model (Support Vector Classifier - SVC)
#svm_model = SVC(kernel='linear')  # You can choose different kernel functions like 'linear', 'rbf', 'poly', etc.

# 5. Train the model on the training data
#svm_model.fit(X_train, y_train)

# 6. Make predictions on the test data
#y_pred = svm_model.predict(X_test)

# 7. Evaluate the model
#accuracy = accuracy_score(y_test, y_pred)
#print("Accuracy:", accuracy)